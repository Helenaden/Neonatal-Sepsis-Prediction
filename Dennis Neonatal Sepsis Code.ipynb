{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neonatal sepsis prediction model using Support Vector Machine (linear and non-linear), Logistic regression, K-nearest neighbor, Naïve bayes and Decision tree\n",
    "#libraries for dataframe\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries for plots\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries for preprocessing and validation\n",
    "from sklearn import preprocessing\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries for models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries for evaluation\n",
    "#Confusion matrix, Accuracy, sensitivity and specificity\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from itertools import cycle\n",
    "from scipy import interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "data=pd.read_csv('/Users/Helen/Desktop/Thesis/Dennis_Neonatal_Sepsis_D.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis of data\n",
    "data=pd.read_csv('/Users/Helen/Desktop/Thesis/Dennis_Neonatal_Sepsis_D.csv')\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of categorical variables\n",
    "s = (data.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to print the categorical variables\n",
    "obj_data = data.select_dtypes(include=['object']).copy()\n",
    "obj_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print numeric features\n",
    "numeric_features = data.select_dtypes(exclude=[object]).columns.values\n",
    "print (numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get total number of the values missing in each variable\n",
    "total = data.isnull().sum().sort_values(ascending=False)\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get the percentage of the missing values in each variable\n",
    "percent = (data.isnull().sum() / data.isnull().count()).sort_values(ascending=False)\n",
    "percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get the total number of missing values and percent of the missing values for each variable\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Missing', 'Percent'])\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values using mean\n",
    "data['platelet_count'] = data['platelet_count'].fillna(data['platelet_count'].mean())\n",
    "data['wbc'] = data['wbc'].fillna(data['wbc'].mean())\n",
    "data['duration_of_labour'] = data['duration_of_labour'].fillna(data['duration_of_labour'].mean())\n",
    "data['duration_of_ROM'] = data['duration_of_ROM'].fillna(data['duration_of_ROM'].mean())\n",
    "data['heart_rate'] = data['heart_rate'].fillna(data['heart_rate'].mean())\n",
    "data['respiratory_rate'] = data['respiratory_rate'].fillna(data['respiratory_rate'].mean())\n",
    "data['neut_count'] = data['neut_count'].fillna(data['neut_count'].mean())\n",
    "#data['lym_count'] = data['lym_count'].fillna(data['lym_count'].mean())\n",
    "#data['mon_count'] = data['mon_count'].fillna(data['mon_count'].mean())\n",
    "#data['eos_count'] = data['eos_count'].fillna(data['eos_count'].mean())\n",
    "#data['bas_count'] = data['bas_count'].fillna(data['bas_count'].mean())\n",
    "#data['rbc'] = data['rbc'].fillna(data['rbc'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to count the missing values in each column\n",
    "data.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get a list of column names containing NaNs(missing values)\n",
    "data.columns[data.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentage_missing(series):\n",
    "    \"\"\" Calculates percentage of NaN values in DataFrame\n",
    "    :param series: Pandas DataFrame object\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    num = series.isnull().sum()\n",
    "    den = len(series)\n",
    "    return round(num/den, 2)\n",
    "\n",
    "# Only include columns that contain any NaN values\n",
    "data_with_any_null_values = data[data.columns[data.isnull().any()].tolist()]\n",
    "\n",
    "get_percentage_missing(data_with_any_null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(data.dtypes.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to print the numerical variables\n",
    "data_num = data.select_dtypes(include = ['float64', 'int64'])\n",
    "data_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots graph of numeric variables\n",
    "data_num.hist(figsize=(8, 8));\n",
    "sv_lab = 'no neontal sepsis'\n",
    "nsv_lab = 'neonatal sepsis'\n",
    "sns.set(color_codes=True)\n",
    "SMALL_SIZE = 10\n",
    "plt.rc('legend', fontsize=SMALL_SIZE) \n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(12,12))\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 1].age_days, bins=20, label=sv_lab, ax=axes[0][0])\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 0].age_days, bins=20, label=nsv_lab, ax=axes[0][0])\n",
    "ax.legend()\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 1].gest_age, bins=20, label=sv_lab, ax=axes[0][1])\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 0].gest_age, bins=20, label=nsv_lab, ax=axes[0][1])\n",
    "ax.legend()\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 1].duration_of_labour, bins=20, label=sv_lab, ax=axes[0][2])\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 0].duration_of_labour, bins=20, label=nsv_lab, ax=axes[0][2])\n",
    "ax.legend()\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 1].duration_of_ROM, bins=20, label=sv_lab, ax=axes[0][3])\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 0].duration_of_ROM, bins=20, label=nsv_lab, ax=axes[0][3])\n",
    "ax.legend()\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 1].weight, bins=20, label=sv_lab, ax=axes[1][0])\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 0].weight, bins=20, label=nsv_lab, ax=axes[1][0])\n",
    "ax.legend()\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 1].temperature, bins=20, label=sv_lab, ax=axes[1][1])\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 0].temperature, bins=20, label=nsv_lab, ax=axes[1][1])\n",
    "ax.legend()\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 1].respiratory_rate, bins=20, label=sv_lab, ax=axes[1][2])\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 0].respiratory_rate, bins=20, label=nsv_lab, ax=axes[1][2])\n",
    "ax.legend()\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 1].heart_rate, bins=20, label=sv_lab, ax=axes[1][3])\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 0].heart_rate, bins=20, label=nsv_lab, ax=axes[1][3])\n",
    "ax.legend()\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 1].wbc, bins=20, label=sv_lab, ax=axes[2][0])\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 0].wbc, bins=20, label=nsv_lab, ax=axes[2][0])\n",
    "ax.legend()\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 1].platelet_count, bins=20, label=sv_lab, ax=axes[2][1])\n",
    "ax = sns.distplot(data_num[data_num['neonatal_sepsis']== 0].platelet_count, bins=20, label=nsv_lab, ax=axes[2][1])\n",
    "ax.legend()\n",
    "plt.xlabel('neut_count', fontsize=14)\n",
    "ax = sns.kdeplot(data_num[data_num['neonatal_sepsis']== 1].neut_count, bw=0.5, label=sv_lab, ax=axes[2][2])\n",
    "ax = sns.kdeplot(data_num[data_num['neonatal_sepsis']== 0].neut_count, bw=0.5, label=nsv_lab, ax=axes[2][2])\n",
    "ax.legend()\n",
    "#ax = sns.kdeplot(data_num[data_num['neonatal_sepsis']== 1].rbc, bw=0.5, label=sv_lab, ax=axes[2][3])\n",
    "#ax = sns.kdeplot(data_num[data_num['neonatal_sepsis']== 0].rbc, bw=0.5, label=nsv_lab, ax=axes[2][3])\n",
    "#plt.xlabel('rbc', fontsize=14)\n",
    "#ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots graph of categorical variables\n",
    "color = ['blue', 'green']\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 10))\n",
    "sns.countplot('neonatal_sepsis', data=data, ax=axes[0, 0], alpha = 0.5)\n",
    "sns.countplot('gender', hue='neonatal_sepsis', data=data, ax=axes[0, 1], alpha = 0.5)\n",
    "sns.countplot('maternal_febrile', hue='neonatal_sepsis', data=data, ax=axes[0, 2], alpha = 0.5)\n",
    "sns.countplot('fever_during_labour', hue='neonatal_sepsis', data=data, ax=axes[0, 3], alpha = 0.5)\n",
    "sns.countplot('abnormal_vag_discharge', hue='neonatal_sepsis', data=data, ax=axes[1, 0], alpha = 0.5)\n",
    "sns.countplot('antibiotic_given', hue='neonatal_sepsis', data=data, ax=axes[1, 1], alpha = 0.5)\n",
    "sns.countplot('place_of_delivery', hue='neonatal_sepsis', data=data, ax=axes[1, 2], alpha = 0.5)\n",
    "sns.countplot('mode_of_delivery', hue='neonatal_sepsis', data=data, ax=axes[1, 3], alpha = 0.5)\n",
    "sns.countplot('rupture_of_mem', hue='neonatal_sepsis', data=data, ax=axes[2, 0], alpha = 0.5)\n",
    "sns.countplot('foul_smelling_liquor', hue='neonatal_sepsis', data=data, ax=axes[2, 1], alpha = 0.5)\n",
    "sns.countplot('fever', hue='neonatal_sepsis', data=data, ax=axes[2, 2], alpha = 0.5)\n",
    "sns.countplot('cold_body', hue='neonatal_sepsis', data=data, ax=axes[2, 3], alpha = 0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots graph of categorical variables\n",
    "color = ['blue', 'green']\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 10))\n",
    "sns.countplot('poor_feeding', hue='neonatal_sepsis', data=data, ax=axes[0, 0], alpha = 0.5)\n",
    "sns.countplot('crying_excessively', hue='neonatal_sepsis', data=data, ax=axes[0, 1], alpha = 0.5)\n",
    "sns.countplot('weak_cry', hue='neonatal_sepsis', data=data, ax=axes[0, 2], alpha = 0.5)\n",
    "sns.countplot('lethargy', hue='neonatal_sepsis', data=data, ax=axes[0, 3], alpha = 0.5)\n",
    "sns.countplot('respiratory_difficulty', hue='neonatal_sepsis', data=data, ax=axes[1, 0], alpha = 0.5)\n",
    "sns.countplot('respiratory_distress', hue='neonatal_sepsis', data=data, ax=axes[1, 1], alpha = 0.5)\n",
    "sns.countplot('tachypnoea', hue='neonatal_sepsis', data=data, ax=axes[1, 2], alpha = 0.5)\n",
    "sns.countplot('apnoea', hue='neonatal_sepsis', data=data, ax=axes[1, 3], alpha = 0.5)\n",
    "sns.countplot('crp_count', hue='neonatal_sepsis', data=data, ax=axes[2, 0], alpha = 0.5)\n",
    "sns.countplot('blood_culture', hue='neonatal_sepsis', data=data, ax=axes[2, 1], alpha = 0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap matix to represent correlation between neonatal sepsis and other features\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "# Compute the correlation matrix\n",
    "correlation = data.select_dtypes(include=['float64','int64']).iloc[:, 1:].corr()\n",
    "sns.heatmap(correlation, ax=ax, vmax=1, annot =True, annot_kws={'size': 7}, square=True)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=360)\n",
    "plt.title('Correlation matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numpy’s tril() function to extract Lower Triangle Matrix\n",
    "np.tril(np.ones(correlation.shape)).astype(np.bool)[0:5,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract lower triangular correlation matrix using pandas’ where() function\n",
    "data_lt = correlation.where(np.tril(np.ones(correlation.shape)).astype(np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upper triangular matrix has NaN and lower triangular matrix has correlation values.\n",
    "data_lt.iloc[0:5,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get lower triangular correlation heatmap\n",
    "hmap=sns.heatmap(data_lt,annot=True,cmap=\"Spectral\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num_corr = data_num.corr()['neonatal_sepsis'][:-1] # -1 because the latest row is SalePrice\n",
    "golden_list = data_num_corr[abs(data_num_corr) > 0.5].sort_values(ascending=False)\n",
    "print(\"There is {} strongly correlated values with neonatal_sepsis:\\n{}\".format(len(golden_list), golden_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var = 'neonatal_sepsis'\n",
    "corr = data.corr()[target_var]\n",
    "correlation = (corr[np.argsort(corr, axis=0)[::-1]])\n",
    "plt.figure(figsize=(10, 10))\n",
    "correlation.plot(kind=\"barh\", fontsize=10, color = 'r')\n",
    "plt.title('Positive and Negative correlation with Target: Neonatal Sepsis')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot of numeric variables with target variable\n",
    "sns.set(style=\"ticks\", color_codes=True, font_scale=0.8)\n",
    "features_mean = ['age_days', 'gest_age', 'duration_of_labour', 'duration_of_ROM', 'weight', 'neonatal_sepsis']\n",
    "sns.pairplot(data[features_mean], hue='neonatal_sepsis', markers=[\"o\", \"s\"], palette='husl', height=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot of numeric variables with target variable\n",
    "#sns.set(style=\"ticks\", color_codes=True, font_scale=0.8)\n",
    "#features_mean_2 = ['temperature', 'respiratory_rate', 'heart_rate', 'wbc', 'neut_count', 'neonatal_sepsis']\n",
    "#sns.pairplot(data[features_mean_2], hue='neonatal_sepsis', markers=[\"o\", \"s\"], palette='husl', height=1.0)\n",
    "#plt.show()\n",
    "#'lym_count',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot of numeric variables with target variable\n",
    "#sns.set(style=\"ticks\", color_codes=True, font_scale=0.8)\n",
    "#features_mean_2 = ['mon_count', 'eos_count', 'bas_count', 'rbc', 'platelet_count', 'neonatal_sepsis']\n",
    "#sns.pairplot(data[features_mean_2], hue='neonatal_sepsis', markers=[\"o\", \"s\"], palette='husl', height=1.0)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate z-score of each numeric variable\n",
    "# normalize all ... except for the target itself!\n",
    "cols = ['age_days', 'gest_age', 'duration_of_labour', 'duration_of_ROM', 'weight', 'temperature', 'respiratory_rate',\n",
    "        'heart_rate', 'wbc', 'neut_count', 'platelet_count']\n",
    "#'lym_count', 'mon_count', 'eos_count', 'bas_count', 'rbc',\n",
    "data[cols] = preprocessing.scale(data[cols]) # scale between 0 and 1.\n",
    "data[cols]\n",
    "#'rbc', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists for categorical variables(columns) to generate the dummy variables,\n",
    "col = ['gender', 'maternal_febrile', 'fever_during_labour', 'abnormal_vag_discharge', 'antibiotic_given', 'place_of_delivery',\n",
    "       'mode_of_delivery', 'rupture_of_mem', 'foul_smelling_liquor', 'fever', 'cold_body', 'poor_feeding', 'crying_excessively',\n",
    "       'weak_cry', 'lethargy', 'respiratory_difficulty', 'respiratory_distress', 'tachypnoea', 'apnoea', 'crp_count',\n",
    "       'blood_culture']\n",
    "data = pd.get_dummies(data, columns=col)\n",
    "print (data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = data.columns.values\n",
    "rfc = RandomForestClassifier()\n",
    "Y = data['neonatal_sepsis']\n",
    "X = data.drop('neonatal_sepsis', 1)\n",
    "rfc.fit(X, Y)\n",
    "    \n",
    "# Print the results\n",
    "importance = rfc.feature_importances_\n",
    "sorted_importances = np.argsort(importance)\n",
    "features = np.arange(len(names)-1)\n",
    "print(\"Features sorted by their score:\")\n",
    "print(sorted(zip(map(lambda x: round(x, 4), rfc.feature_importances_), names), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.barh(features, importance[sorted_importances], align='center', color = 'green', alpha = 0.5)\n",
    "plt.yticks(features, names[sorted_importances])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.title(\"Variable Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (data.neonatal_sepsis.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['neonatal_sepsis']\n",
    "x = data.drop('neonatal_sepsis', 1)\n",
    "oversampler = SMOTE(random_state=15)\n",
    "X_bal, y_bal = oversampler.fit_sample(x, y)\n",
    "print(X_bal, y_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_bal.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = y_bal\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#support vector machine linear model\n",
    "parameters = {'kernel': ['linear'], 'C':[1,2,3,4,5,6,7,8,9,10], 'gamma':[0.01,0.02,0.03,0.04,0.05,0.10,0.2,0.3,0.4,0.5]}\n",
    "seed = 7\n",
    "model_svml = svm.SVC()\n",
    "grid = GridSearchCV(model_svml , parameters)\n",
    "skf = StratifiedKFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "skf.get_n_splits(X_bal, y_bal)\n",
    "#ROC curve\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange', 'red', 'brown', 'green', 'grey'])\n",
    "lw = 2\n",
    "i = 0\n",
    "auc_svml = {}\n",
    "for (train_index, test_index), color in zip (skf.split(X_bal, y_bal), colors):\n",
    "    X_trainN, X_testN = X_bal.iloc[train_index], X_bal.iloc[test_index]\n",
    "    y_trainN, y_testN = y_bal[train_index], y_bal[test_index]\n",
    "    grid.fit(X_trainN, y_trainN)\n",
    "    pred_svml = grid.predict(X_testN)\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_bal[test_index], pred_svml)\n",
    "    mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=lw, color=color, label='ROC fold %d (area = %0.2f)' % (i + 1, roc_auc))\n",
    "    auc_svml[i] = roc_auc\n",
    "    i += 1\n",
    "print(auc_svml)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k')\n",
    "mean_tpr /= skf.get_n_splits(X_bal, y_bal)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc_svml = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--', label='Mean ROC (area = %0.2f)' % mean_auc_svml, lw=lw)\n",
    "print('Average SVM Linear classifer accuracy = %0.2f' % (mean_auc_svml*100.0))\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve: Support Vector Machine - Linear')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm_l = metrics.confusion_matrix(y_bal[test_index],pred_svml)\n",
    "# Assigning columns names\n",
    "cm_svml = pd.DataFrame(cm_l, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_svml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 36\n",
    "TN = 35\n",
    "FP = 3\n",
    "FN = 1\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted probabilities for class 1 (septic)\n",
    "parameters = {'kernel': ['linear'], 'C':[1,2,3,4,5,6,7,8,9,10], 'gamma':[0.01,0.02,0.03,0.04,0.05,0.10,0.2,0.3,0.4,0.5]}\n",
    "model_svml2 = svm.SVC(probability=True)\n",
    "grid = GridSearchCV(model_svml2 , parameters)\n",
    "grid.fit(X_trainN, y_trainN)\n",
    "y_pred_prob = grid.predict_proba(X_testN)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict sepsis if the predicted probability is greater than 0.11\n",
    "from sklearn.preprocessing import binarize\n",
    "threshold = 0.11\n",
    "y_pred_classl = binarize([y_pred_prob], threshold)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm_l1 = metrics.confusion_matrix(y_bal[test_index],y_pred_classl)\n",
    "# Assigning columns names\n",
    "cm_svml1 = pd.DataFrame(cm_l1, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_svml1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 36\n",
    "TN = 30\n",
    "FP = 8\n",
    "FN = 1\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_svml\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = y_pred_classl\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_svmlr = np.array([[66, 0],\n",
    "                     [5, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import mcnemar\n",
    "\n",
    "chi2, p = mcnemar(ary=tb_svmlr, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import mcnemar\n",
    "\n",
    "chi2, p = mcnemar(ary=tb_svmlr, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_svml = np.array([[29, 9],\n",
    "                    [1, 36]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing mcnemar_table and mcnemar\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "from mlxtend.evaluate import mcnemar\n",
    "chi2, p = mcnemar(ary=tb_svml, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_svml, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict sepsis if the predicted probability is greater than 0.88\n",
    "from sklearn.preprocessing import binarize\n",
    "threshold = 0.88\n",
    "y_pred_classl2 = binarize([y_pred_prob], threshold)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm_l2 = metrics.confusion_matrix(y_bal[test_index],y_pred_classl2)\n",
    "# Assigning columns names\n",
    "cm_svml2 = pd.DataFrame(cm_l2, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_svml2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 33\n",
    "TN = 37\n",
    "FP = 1\n",
    "FN = 4\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_svml\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = y_pred_classl2\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_svmlr1 = np.array([[68, 2],\n",
    "                      [3, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_svmlr1, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_svml1 = np.array([[37, 1],\n",
    "                     [4, 33]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_svml1, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_svml1, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#support vector machine radial basis function model\n",
    "parameters = {'kernel': ['rbf'], 'C': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'gamma': \n",
    "              [0.01, 0.02, 0.03, 0.04, 0.05, 0.10, 0.2, 0.3, 0.4, 0.5]}\n",
    "seed = 7\n",
    "model_svmrbf = svm.SVC()\n",
    "grid = GridSearchCV(model_svmrbf, parameters)\n",
    "skf = StratifiedKFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "skf.get_n_splits(X_bal, y_bal)\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange', 'red', 'brown', 'green', 'grey'])\n",
    "lw = 2\n",
    "i = 0\n",
    "auc_svmrbf = {}\n",
    "for (train_index, test_index), color in zip (skf.split(X_bal, y_bal), colors):\n",
    "    X_trainN, X_testN = X_bal.iloc[train_index], X_bal.iloc[test_index]\n",
    "    y_trainN, y_testN = y_bal[train_index], y_bal[test_index]\n",
    "    grid.fit(X_trainN, y_trainN)\n",
    "    pred_svmrbf = grid.predict(X_testN)\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_bal[test_index], pred_svmrbf)\n",
    "    mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=lw, color=color, label='ROC fold %d (area = %0.2f)' % (i + 1, roc_auc))\n",
    "    auc_svmrbf[i] = roc_auc\n",
    "    i += 1\n",
    "print(auc_svmrbf)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k')\n",
    "mean_tpr /= skf.get_n_splits(X_bal, y_bal)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc_svmrbf = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--', label='Mean ROC (area = %0.2f)' % mean_auc_svmrbf, lw=lw)\n",
    "print('Average SVM Radial classifer accuracy = %0.2f' % (mean_auc_svmrbf*100.0))\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve: Support Vector Machine - Radial')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm_rbf = metrics.confusion_matrix(y_bal[test_index],pred_svmrbf)\n",
    "# Assigning columns names\n",
    "cm_rbf = pd.DataFrame(cm_rbf, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 35\n",
    "TN = 36\n",
    "FP = 2\n",
    "FN = 2\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted probabilities for class 1 (septic)\n",
    "parameters = {'kernel': ['rbf'], 'C': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'gamma': \n",
    "              [0.01, 0.02, 0.03, 0.04, 0.05, 0.10, 0.2, 0.3, 0.4, 0.5]}\n",
    "model_svmrbf2 = svm.SVC(probability=True)\n",
    "grid = GridSearchCV(model_svmrbf2, parameters)\n",
    "grid.fit(X_trainN, y_trainN)\n",
    "y_pred_prob2 = grid.predict_proba(X_testN)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict sepsis if the predicted probability is greater than 0.11\n",
    "from sklearn.preprocessing import binarize\n",
    "threshold = 0.11\n",
    "y_pred_class_rbfl = binarize([y_pred_prob2], threshold)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm_rbf1 = metrics.confusion_matrix(y_bal[test_index],y_pred_class_rbfl)\n",
    "# Assigning columns names\n",
    "cm_rbf1 = pd.DataFrame(cm_rbf1, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_rbf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 37\n",
    "TN = 36\n",
    "FP = 2\n",
    "FN = 0\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_svmrbf\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = y_pred_class_rbfl \n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_rbfn = np.array([[71, 2],\n",
    "                    [0, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_rbfn, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_rbf = np.array([[35, 3],\n",
    "                   [0, 37]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_rbf, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_rbf, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict sepsis if the predicted probability is greater than 0.88\n",
    "from sklearn.preprocessing import binarize\n",
    "threshold = 0.88\n",
    "y_pred_class_rbf2 = binarize([y_pred_prob2], threshold)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm_rbf2 = metrics.confusion_matrix(y_bal[test_index],y_pred_class_rbf2)\n",
    "# Assigning columns names\n",
    "cm_rbf2 = pd.DataFrame(cm_rbf2, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_rbf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 35\n",
    "TN = 36\n",
    "FP = 2\n",
    "FN = 2\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_svmrbf\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = y_pred_class_rbf2 \n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_rbfn2 = np.array([[71, 0],\n",
    "                     [0, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_rbfn2, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_rbfn2, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_rbf2 = np.array([[36, 2],\n",
    "                    [2, 35]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_rbf2, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_rbf2, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#support vector machine polynomial model\n",
    "parameters = {'kernel': ['poly'], 'C': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'gamma': \n",
    "              [0.01, 0.02, 0.03, 0.04, 0.05, 0.10, 0.2, 0.3, 0.4, 0.5]}\n",
    "seed = 7\n",
    "model_svmpoly = svm.SVC()\n",
    "grid = GridSearchCV(model_svmpoly, parameters)\n",
    "skf = StratifiedKFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "skf.get_n_splits(X_bal, y_bal)\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange', 'red', 'brown', 'green', 'grey'])\n",
    "lw = 2\n",
    "i = 0\n",
    "auc_svmpoly = {}\n",
    "for (train_index, test_index), color in zip (skf.split(X_bal, y_bal), colors):\n",
    "    X_trainN, X_testN = X_bal.iloc[train_index], X_bal.iloc[test_index]\n",
    "    y_trainN, y_testN = y_bal[train_index], y_bal[test_index]\n",
    "    grid.fit(X_trainN, y_trainN)\n",
    "    pred_svmpoly = grid.predict(X_testN)\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_bal[test_index], pred_svmpoly)\n",
    "    mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=lw, color=color, label='ROC fold %d (area = %0.2f)' % (i + 1, roc_auc))\n",
    "    auc_svmpoly[i] = roc_auc\n",
    "    i += 1\n",
    "print(auc_svmpoly)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k')\n",
    "mean_tpr /= skf.get_n_splits(X_bal, y_bal)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc_svmpoly = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--', label='Mean ROC (area = %0.2f)' % mean_auc_svmpoly, lw=lw)\n",
    "print('Average SVM Poly classifer accuracy = %0.2f' % (mean_auc_svmpoly*100.0))\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve: Support Vector Machine - Poly')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm_poly = metrics.confusion_matrix(y_bal[test_index],pred_svmpoly)\n",
    "# Assigning columns names\n",
    "cm_df_poly = pd.DataFrame(cm_poly, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 35\n",
    "TN = 37\n",
    "FP = 1\n",
    "FN = 2\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted probabilities for class 1 (septic)\n",
    "parameters = {'kernel': ['poly'], 'C': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'gamma': \n",
    "              [0.01, 0.02, 0.03, 0.04, 0.05, 0.10, 0.2, 0.3, 0.4, 0.5]}\n",
    "model_svmpoly2 = svm.SVC(probability=True)\n",
    "grid = GridSearchCV(model_svmpoly2, parameters)\n",
    "grid.fit(X_trainN, y_trainN)\n",
    "y_pred_prob3 = grid.predict_proba(X_testN)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict sepsis if the predicted probability is greater than 0.11\n",
    "from sklearn.preprocessing import binarize\n",
    "threshold = 0.11\n",
    "y_pred_class_poly1 = binarize([y_pred_prob3], threshold)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm_poly1 = metrics.confusion_matrix(y_bal[test_index],y_pred_class_poly1)\n",
    "# Assigning columns names\n",
    "cm_df_poly1 = pd.DataFrame(cm_poly1, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df_poly1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 37\n",
    "TN = 35\n",
    "FP = 3\n",
    "FN = 0\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_svmpoly\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = y_pred_class_poly1\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_polyn = np.array([[70, 2],\n",
    "                    [2, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_polyn, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_poly = np.array([[35, 3],\n",
    "                   [0, 37]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_poly, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_poly, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict sepsis if the predicted probability is greater than 0.88\n",
    "from sklearn.preprocessing import binarize\n",
    "threshold = 0.88\n",
    "y_pred_class_poly2 = binarize([y_pred_prob3], threshold)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm_poly2 = metrics.confusion_matrix(y_bal[test_index],y_pred_class_poly2)\n",
    "# Assigning columns names\n",
    "cm_df_poly2 = pd.DataFrame(cm_poly2, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df_poly2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 35\n",
    "TN = 37\n",
    "FP = 1\n",
    "FN = 2\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_svmpoly\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = y_pred_class_poly2\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_polyn2 = np.array([[72, 0],\n",
    "                      [0, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_polyn2, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_poly2 = np.array([[37, 1],\n",
    "                     [2, 35]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_poly2, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_poly2, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression model\n",
    "seed = 7\n",
    "model_lr = LogisticRegression()\n",
    "skf = StratifiedKFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "skf.get_n_splits(X_bal, y_bal)\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange', 'red', 'brown', 'green', 'grey'])\n",
    "lw = 2\n",
    "i = 0\n",
    "auc_lr = {}\n",
    "for (train_index, test_index), color in zip (skf.split(X_bal, y_bal), colors):\n",
    "    X_trainN, X_testN = X_bal.iloc[train_index], X_bal.iloc[test_index]\n",
    "    y_trainN, y_testN = y_bal[train_index], y_bal[test_index]\n",
    "    model_lr.fit(X_trainN, y_trainN)\n",
    "    pred_lr = model_lr.predict(X_testN)\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_bal[test_index], pred_lr)\n",
    "    mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=lw, color=color, label='ROC fold %d (area = %0.2f)' % (i + 1, roc_auc))\n",
    "    auc_lr[i] = roc_auc\n",
    "    i += 1\n",
    "print(auc_lr)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k')\n",
    "mean_tpr /= skf.get_n_splits(X_bal, y_bal)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc_lr = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--', label='Mean ROC (area = %0.2f)' % mean_auc_lr, lw=lw)\n",
    "print('Average LR classifer accuracy = %0.2f' % (mean_auc_lr*100.0))\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve: Logistic regression')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm_lr = metrics.confusion_matrix(y_bal[test_index],pred_lr)\n",
    "# Assigning columns names\n",
    "cm_df_lr = pd.DataFrame(cm_lr, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 33\n",
    "TN = 35\n",
    "FP = 3\n",
    "FN = 4\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted probabilities for class 1 (septic)\n",
    "y_pred_prob4 = model_lr.predict_proba(X_testN)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict sepsis if the predicted probability is greater than 0.11\n",
    "from sklearn.preprocessing import binarize\n",
    "threshold = 0.11\n",
    "y_pred_class_lr = binarize([y_pred_prob4], threshold)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm_lr1 = metrics.confusion_matrix(y_bal[test_index],y_pred_class_lr)\n",
    "# Assigning columns names\n",
    "cm_df_lr1 = pd.DataFrame(cm_lr1, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df_lr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 36\n",
    "TN = 33\n",
    "FP = 5\n",
    "FN = 1\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_lr\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = y_pred_class_lr\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_lrn = np.array([[66, 3],\n",
    "                   [2, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_lrn, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_lrn, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_lr = np.array([[33, 5],\n",
    "                  [1, 36]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_lr, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_lr, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict sepsis if the predicted probability is greater than 0.89\n",
    "from sklearn.preprocessing import binarize\n",
    "threshold = 0.89\n",
    "y_pred_class_lr2 = binarize([y_pred_prob4], threshold)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm_lr2 = metrics.confusion_matrix(y_bal[test_index],y_pred_class_lr2)\n",
    "# Assigning columns names\n",
    "cm_df_lr2 = pd.DataFrame(cm_lr2, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df_lr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 31\n",
    "TN = 36\n",
    "FP = 2\n",
    "FN = 6\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_lr\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = y_pred_class_lr2\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_lrn2 = np.array([[66, 1],\n",
    "                    [2, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_lrn2, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_lr2 = np.array([[36, 2],\n",
    "                   [6, 31]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_lr2, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_lr, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-nearest_neighbor model\n",
    "seed = 7\n",
    "model_neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "skf = StratifiedKFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "skf.get_n_splits(X_bal, y_bal)\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange', 'red', 'brown', 'green', 'grey'])\n",
    "lw = 2\n",
    "i = 0\n",
    "auc_neigh = {}\n",
    "for (train_index, test_index), color in zip (skf.split(X_bal, y_bal), colors):\n",
    "    X_trainN, X_testN = X_bal.to_numpy()[train_index], X_bal.to_numpy()[test_index]\n",
    "    y_trainN, y_testN = y_bal[train_index], y_bal[test_index]\n",
    "    model_neigh.fit(X_trainN, y_trainN)\n",
    "    pred_neigh = model_neigh.predict(X_testN)\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_bal[test_index], pred_neigh)\n",
    "    mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=lw, color=color, label='ROC fold %d (area = %0.2f)' % (i + 1, roc_auc))\n",
    "    auc_neigh[i] = roc_auc\n",
    "    i += 1\n",
    "print(auc_neigh)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k')\n",
    "mean_tpr /= skf.get_n_splits(X_bal, y_bal)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc_neigh = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--', label='Mean ROC (area = %0.2f)' % mean_auc_neigh, lw=lw)\n",
    "print('Average KNN classifer accuracy = %0.2f' % (mean_auc_neigh*100.0))\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve: K-nearest neighbor')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the metrics package from sklearn library\n",
    "from sklearn import metrics\n",
    "# Creating the confusion matrix\n",
    "cm_neigh = metrics.confusion_matrix(y_bal[test_index],pred_neigh)\n",
    "# Assigning columns names\n",
    "cm_df_neigh = pd.DataFrame(cm_neigh, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df_neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 31\n",
    "TN = 37\n",
    "FP = 1\n",
    "FN = 6\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted probabilities for class 1 (septic)\n",
    "y_pred_prob5 = model_neigh.predict_proba(X_testN)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict sepsis if the predicted probability is greater than 0.11\n",
    "from sklearn.preprocessing import binarize\n",
    "threshold = 0.11\n",
    "y_pred_class_neigh = binarize([y_pred_prob5], threshold)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm_neigh1 = metrics.confusion_matrix(y_bal[test_index],y_pred_class_neigh)\n",
    "# Assigning columns names\n",
    "cm_df_neigh1 = pd.DataFrame(cm_neigh1, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df_neigh1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 34\n",
    "TN = 36\n",
    "FP = 2\n",
    "FN = 3\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_neigh\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = y_pred_class_neigh\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_neighn = np.array([[67, 3],\n",
    "                      [1, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_neighn, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_neigh = np.array([[36, 2],\n",
    "                     [3, 34]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_neigh, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_neigh, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict sepsis if the predicted probability is greater than 0.89\n",
    "from sklearn.preprocessing import binarize\n",
    "threshold = 0.89\n",
    "y_pred_class_neigh2 = binarize([y_pred_prob5], threshold)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm3 = metrics.confusion_matrix(y_bal[test_index],y_pred_class_neigh2)\n",
    "# Assigning columns names\n",
    "cm_df3 = pd.DataFrame(cm3, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 25\n",
    "TN = 38\n",
    "FP = 0\n",
    "FN = 12\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_neigh\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = y_pred_class_neigh2\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_neighn2 = np.array([[62, 1],\n",
    "                      [6, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_neighn2, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_neighn2, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_neigh2 = np.array([[38, 0],\n",
    "                      [12, 25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_neigh2, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_neigh2, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian Naive Bayes (GaussianNB) model\n",
    "seed = 7\n",
    "model_NB = GaussianNB()\n",
    "skf = StratifiedKFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "skf.get_n_splits(X_bal, y_bal)\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange', 'red', 'brown', 'green', 'grey'])\n",
    "lw = 2\n",
    "i = 0\n",
    "auc_NB = {}\n",
    "for (train_index, test_index), color in zip (skf.split(X_bal, y_bal), colors):\n",
    "    X_trainN, X_testN = X_bal.iloc[train_index], X_bal.iloc[test_index]\n",
    "    y_trainN, y_testN = y_bal[train_index], y_bal[test_index]\n",
    "    model_NB.fit(X_trainN, y_trainN)\n",
    "    pred_NB = model_NB.predict(X_testN)\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_bal[test_index], pred_NB)\n",
    "    mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=lw, color=color, label='ROC fold %d (area = %0.2f)' % (i + 1, roc_auc))\n",
    "    auc_NB[i] = roc_auc\n",
    "    i += 1\n",
    "print(auc_NB)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k')\n",
    "mean_tpr /= skf.get_n_splits(X_bal, y_bal)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc_NB = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--', label='Mean ROC (area = %0.2f)' % mean_auc_NB, lw=lw)\n",
    "print('Average GaussianNB classifer accuracy = %0.2f' % (mean_auc_NB*100.0))\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve: Gaussian Naive Bayes')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm_NB = metrics.confusion_matrix(y_bal[test_index],pred_NB)\n",
    "# Assigning columns names\n",
    "cm_df_NB = pd.DataFrame(cm_NB, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 35\n",
    "TN = 36\n",
    "FP = 2\n",
    "FN = 2\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted probabilities for class 1 (septic)\n",
    "y_pred_prob6 = model_NB.predict_proba(X_testN)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict sepsis if the predicted probability is greater than 0.11\n",
    "from sklearn.preprocessing import binarize\n",
    "threshold = 0.11\n",
    "y_pred_class_NB = binarize([y_pred_prob6], threshold)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm3 = metrics.confusion_matrix(y_bal[test_index],y_pred_class_NB)\n",
    "# Assigning columns names\n",
    "cm_df3 = pd.DataFrame(cm3, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 35\n",
    "TN = 36\n",
    "FP = 2\n",
    "FN = 2\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_NB\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = y_pred_class_NB\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_NBn = np.array([[71, 0],\n",
    "                   [0, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_NBn, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_NB = np.array([[36, 2],\n",
    "                  [2, 35]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_NB, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_NB, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict sepsis if the predicted probability is greater than 0.89\n",
    "from sklearn.preprocessing import binarize\n",
    "threshold = 0.89\n",
    "y_pred_class_NB2 = binarize([y_pred_prob6], threshold)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm3 = metrics.confusion_matrix(y_bal[test_index],y_pred_class_NB2)\n",
    "# Assigning columns names\n",
    "cm_df3 = pd.DataFrame(cm3, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 35\n",
    "TN = 37\n",
    "FP = 1\n",
    "FN = 2\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_NB\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = y_pred_class_NB2\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_NBn2 = np.array([[71, 1],\n",
    "                    [0, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_NBn2, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_NB2, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_NB2, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree model\n",
    "seed = 7\n",
    "model_DT = DecisionTreeClassifier()\n",
    "skf = StratifiedKFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "skf.get_n_splits(X_bal, y_bal)\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange', 'red', 'brown', 'green', 'grey'])\n",
    "lw = 2\n",
    "i = 0\n",
    "auc_DT = {}\n",
    "for (train_index, test_index), color in zip (skf.split(X_bal, y_bal), colors):\n",
    "    X_trainN, X_testN = X_bal.iloc[train_index], X_bal.iloc[test_index]\n",
    "    y_trainN, y_testN = y_bal[train_index], y_bal[test_index]\n",
    "    model_DT.fit(X_trainN, y_trainN)\n",
    "    pred_DT = model_DT.predict(X_testN)\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_bal[test_index], pred_DT)\n",
    "    mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=lw, color=color, label='ROC fold %d (area = %0.2f)' % (i + 1, roc_auc))\n",
    "    auc_DT[i] = roc_auc\n",
    "    i += 1\n",
    "print(auc_DT)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k')\n",
    "mean_tpr /= skf.get_n_splits(X_bal, y_bal)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc_DT = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--', label='Mean ROC (area = %0.2f)' % mean_auc_DT, lw=lw)\n",
    "print('Average DT classifer accuracy = %0.2f' % (mean_auc_DT*100.0))\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve: Decision Tree')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm_DT = metrics.confusion_matrix(y_bal[test_index],pred_DT)\n",
    "# Assigning columns names\n",
    "cm_df_DT = pd.DataFrame(cm_DT, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df_DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 36\n",
    "TN = 36\n",
    "FP = 2\n",
    "FN = 1\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted probabilities for class 1 (septic)\n",
    "y_pred_prob7 =  model_DT.predict_proba(X_testN)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict sepsis if the predicted probability is greater than 0.11\n",
    "from sklearn.preprocessing import binarize\n",
    "threshold = 0.11\n",
    "y_pred_class_DT = binarize([y_pred_prob7], threshold)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm_DT2 = metrics.confusion_matrix(y_bal[test_index],y_pred_class_DT)\n",
    "# Assigning columns names\n",
    "cm_df_DT2 = pd.DataFrame(cm_DT2, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df_DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 36\n",
    "TN = 36\n",
    "FP = 2\n",
    "FN = 1\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_DT\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = y_pred_class_DT\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_DTn = np.array([[72, 0],\n",
    "                   [0, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_DTn, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_DT = np.array([[36, 2],\n",
    "                  [2, 35]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_DT, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_DT, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict sepsis if the predicted probability is greater than 0.89\n",
    "from sklearn.preprocessing import binarize\n",
    "threshold = 0.89\n",
    "y_pred_class_DT2 = binarize([y_pred_prob7], threshold)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the confusion matrix\n",
    "cm_DT3 = metrics.confusion_matrix(y_bal[test_index],y_pred_class_DT2)\n",
    "# Assigning columns names\n",
    "cm_df_DT3 = pd.DataFrame(cm_DT3, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df_DT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "# save confusion matrix and slice into four pieces\n",
    "TP = 35\n",
    "TN = 36\n",
    "FP = 2\n",
    "FN = 1\n",
    "print('True Positives:', TP)\n",
    "print('True Negatives:', TN)\n",
    "print('False Positives:', FP)\n",
    "print('False Negatives:', FN)\n",
    "    \n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "print('Accuracy: ', conf_accuracy) \n",
    "    \n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "print('Mis-Classification: ', conf_misclassification)\n",
    "    \n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "print('Sensitivity: ', conf_sensitivity) \n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "print('Specificity: ', conf_specificity) \n",
    "    \n",
    "# calculate ppv\n",
    "conf_ppv = (TP / float(TP + FP))\n",
    "print('PPV: ', conf_ppv)\n",
    "# calculate npv\n",
    "conf_npv = (TN / float(TN + FN))\n",
    "print('NPV: ', conf_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_DT\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = y_pred_class_DT2\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_DTn2 = np.array([[72, 0],\n",
    "                    [0, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_DTn2, exact=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_DT2 = np.array([[36, 2],\n",
    "                      [2, 35]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_DT2, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 =pred_svml\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_svmrbf\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model = np.array([[69, 2],\n",
    "                     [2, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_model, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 =pred_svml\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_svmpoly\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model1 = np.array([[70, 2],\n",
    "                      [1, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_model1, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 =pred_svml\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_lr\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model2 = np.array([[68, 0],\n",
    "                      [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_model2, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 =pred_svml\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_neigh\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model3 = np.array([[65, 3],\n",
    "                      [6, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb_model3, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 =pred_svml\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_NB\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model4 = np.array([[69, 2],\n",
    "                      [2, 2]])\n",
    "chi2, p = mcnemar(ary=tb_model4, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 =pred_svml\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_DT\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model5 = np.array([[68, 4],\n",
    "                      [3, 0]])\n",
    "chi2, p = mcnemar(ary=tb_model5, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_svmrbf\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_svmpoly\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model6 = np.array([[71, 1],\n",
    "                      [0, 3]])\n",
    "chi2, p = mcnemar(ary=tb_model6, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_svmrbf\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_lr\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model7 = np.array([[66, 2],\n",
    "                      [5, 2]])\n",
    "chi2, p = mcnemar(ary=tb_model7, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_svmrbf\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_neigh\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model8 = np.array([[66, 2],\n",
    "                      [5, 2]])\n",
    "chi2, p = mcnemar(ary=tb_model8, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_svmrbf\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_NB\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model9 = np.array([[69, 2],\n",
    "                      [2, 2]])\n",
    "chi2, p = mcnemar(ary=tb_model9, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_svmrbf\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_DT\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model10 = np.array([[69, 3],\n",
    "                       [2, 1]])\n",
    "chi2, p = mcnemar(ary=tb_model10, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_svmpoly\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_lr\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model11 = np.array([[67, 1],\n",
    "                       [5, 2]])\n",
    "chi2, p = mcnemar(ary=tb_model11, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_svmpoly\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_neigh\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model12 = np.array([[67, 1],\n",
    "                       [5, 2]])\n",
    "chi2, p = mcnemar(ary=tb_model12, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_svmpoly\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_NB\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model13 = np.array([[70, 1],\n",
    "                       [2, 2]])\n",
    "chi2, p = mcnemar(ary=tb_model13, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_svmpoly\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_DT\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model14 = np.array([[69, 3],\n",
    "                       [3, 0]])\n",
    "chi2, p = mcnemar(ary=tb_model14, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_lr\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_neigh\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model15 = np.array([[62, 6],\n",
    "                       [6, 1]])\n",
    "chi2, p = mcnemar(ary=tb_model15, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_lr\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_NB\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model16 = np.array([[67, 4],\n",
    "                       [1, 3]])\n",
    "chi2, p = mcnemar(ary=tb_model16, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_lr\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_DT\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model17 = np.array([[65, 7],\n",
    "                       [3, 0]])\n",
    "chi2, p = mcnemar(ary=tb_model17, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_neigh\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_NB\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model18 = np.array([[65, 6],\n",
    "                       [3, 1]])\n",
    "chi2, p = mcnemar(ary=tb_model18, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_neigh\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_DT\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model19 = np.array([[65, 7],\n",
    "                       [3, 0]])\n",
    "chi2, p = mcnemar(ary=tb_model19, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = y_testN\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = pred_NB\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = pred_DT\n",
    "\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model20 = np.array([[68, 4],\n",
    "                       [3, 0]])\n",
    "chi2, p = mcnemar(ary=tb_model20, exact=False, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_RBF = [auc_svmrbf.get(0), auc_svmrbf.get(1), auc_svmrbf.get(2), auc_svmrbf.get(3), auc_svmrbf.get(4),auc_svmrbf.get(5), \n",
    "           auc_svmrbf.get(6), auc_svmrbf.get(7), auc_svmrbf.get(8), auc_svmrbf.get(9)]\n",
    "SVM_POLY = [auc_svmpoly.get(0), auc_svmpoly.get(1), auc_svmpoly.get(2), auc_svmpoly.get(3), auc_svmpoly.get(4), auc_svmpoly.get(5),\n",
    "            auc_svmpoly.get(6), auc_svmpoly.get(7), auc_svmpoly.get(8), auc_svmpoly.get(9)]\n",
    "LR = [auc_lr.get(0), auc_lr.get(1), auc_lr.get(2), auc_lr.get(3), auc_lr.get(4), auc_lr.get(5), auc_lr.get(6), auc_lr.get(7), \n",
    "      auc_lr.get(8), auc_lr.get(9)]\n",
    "KNN = [auc_neigh.get(0), auc_neigh.get(1), auc_neigh.get(2), auc_neigh.get(3), auc_neigh.get(4), auc_neigh.get(5), \n",
    "      auc_neigh.get(6), auc_neigh.get(7), auc_neigh.get(8), auc_neigh.get(9)]\n",
    "NB = [auc_NB.get(0), auc_NB.get(1), auc_NB.get(2), auc_NB.get(3), auc_NB.get(4), auc_NB.get(5), auc_NB.get(6), auc_NB.get(7), \n",
    "      auc_NB.get(8), auc_NB.get(9)]\n",
    "DT = [auc_DT.get(0), auc_DT.get(1), auc_DT.get(2), auc_DT.get(3), auc_DT.get(4), auc_DT.get(5), auc_DT.get(6), auc_DT.get(7), \n",
    "      auc_DT.get(8), auc_DT.get(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHY1 = [0.11]\n",
    "PHY2 = [0.89]\n",
    "SEN_L1 = [0.97]\n",
    "SPEC_L1 = [0.97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.wilcoxon(PHY1, SEN_L1)\n",
    "print(\"\\n SVML - SVMRBF \")\n",
    "print(t_statistic)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.wilcoxon(PHY2, SPEC_L1)\n",
    "print(\"\\n SVML - SVMRBF \")\n",
    "print(t_statistic)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model comparison\n",
    "np.random.seed(10)\n",
    "SVM_L = [auc_svml.get(0), auc_svml.get(1), auc_svml.get(2), auc_svml.get(3), auc_svml.get(4), auc_svml.get(5), auc_svml.get(6), \n",
    "         auc_svml.get(7), auc_svml.get(8), auc_svml.get(9)]\n",
    "SVM_RBF = [auc_svmrbf.get(0), auc_svmrbf.get(1), auc_svmrbf.get(2), auc_svmrbf.get(3), auc_svmrbf.get(4),auc_svmrbf.get(5), \n",
    "           auc_svmrbf.get(6), auc_svmrbf.get(7), auc_svmrbf.get(8), auc_svmrbf.get(9)]\n",
    "SVM_POLY = [auc_svmpoly.get(0), auc_svmpoly.get(1), auc_svmpoly.get(2), auc_svmpoly.get(3), auc_svmpoly.get(4), auc_svmpoly.get(5),\n",
    "            auc_svmpoly.get(6), auc_svmpoly.get(7), auc_svmpoly.get(8), auc_svmpoly.get(9)]\n",
    "LR = [auc_lr.get(0), auc_lr.get(1), auc_lr.get(2), auc_lr.get(3), auc_lr.get(4), auc_lr.get(5), auc_lr.get(6), auc_lr.get(7), \n",
    "      auc_lr.get(8), auc_lr.get(9)]\n",
    "KNN = [auc_neigh.get(0), auc_neigh.get(1), auc_neigh.get(2), auc_neigh.get(3), auc_neigh.get(4), auc_neigh.get(5), \n",
    "      auc_neigh.get(6), auc_neigh.get(7), auc_neigh.get(8), auc_neigh.get(9)]\n",
    "NB = [auc_NB.get(0), auc_NB.get(1), auc_NB.get(2), auc_NB.get(3), auc_NB.get(4), auc_NB.get(5), auc_NB.get(6), auc_NB.get(7), \n",
    "      auc_NB.get(8), auc_NB.get(9)]\n",
    "DT = [auc_DT.get(0), auc_DT.get(1), auc_DT.get(2), auc_DT.get(3), auc_DT.get(4), auc_DT.get(5), auc_DT.get(6), auc_DT.get(7), \n",
    "      auc_DT.get(8), auc_DT.get(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.wilcoxon(SVM_L, SVM_RBF)\n",
    "print(\"\\n SVML - SVMRBF \")\n",
    "print(t_statistic)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.wilcoxon(SVM_L, SVM_POLY)\n",
    "print(\"\\n SVML - SVM_POLY \")\n",
    "print(t_statistic)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.wilcoxon(SVM_L, LR)\n",
    "print(\"\\n SVML - LR \")\n",
    "print(t_statistic)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.wilcoxon(SVM_L, KNN)\n",
    "print(\"\\n SVML - KNN \")\n",
    "print(t_statistic)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.wilcoxon(SVM_L, NB)\n",
    "print(\"\\n SVML - NB \")\n",
    "print(t_statistic)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.wilcoxon(SVM_L, DT)\n",
    "print(\"\\n SVML - DT \")\n",
    "print(t_statistic)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.wilcoxon(SVM_RBF, SVM_POLY)\n",
    "print(\"\\n SVM_RBF - SVM_POLY \")\n",
    "print(t_statistic)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.wilcoxon(SVM_RBF, LR)\n",
    "print(\"\\n SVM_RBF - LR \")\n",
    "print(t_statistic)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.wilcoxon(SVM_RBF, NB)\n",
    "print(\"\\n SVM_RBF - NB\")\n",
    "print(t_statistic)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.wilcoxon(SVM_RBF, DT)\n",
    "print(\"\\n SVM_RBF - DT \")\n",
    "print(t_statistic)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.wilcoxon(SVM_POLY, LR)\n",
    "print(\"\\n SVM_POLY - LR \")\n",
    "print(t_statistic)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.wilcoxon(SVM_POLY, NB)\n",
    "print(\"\\n SVM_POLY - NB \")\n",
    "print(t_statistic)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.wilcoxon(SVM_POLY, DT)\n",
    "print(\"\\n SVM_POLY - DT \")\n",
    "print(t_statistic)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.wilcoxon(LR, NB)\n",
    "print(\"\\n LR - NB \")\n",
    "print(t_statistic)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.wilcoxon(LR, DT)\n",
    "print(\"\\n LR - DT \")\n",
    "print(t_statistic)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.wilcoxon(NB, DT)\n",
    "print(\"\\n NB - DT \")\n",
    "print(t_statistic)\n",
    "print(p_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
